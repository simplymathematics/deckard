schema: '2.0'
stages:
  train:
    cmd: python -m deckard.layers.experiment train --config_file cifar100.yaml
    params:
      params.yaml:
        data:
          _target_: deckard.base.data.Data
          generate:
            _target_: deckard.base.data.generator.DataGenerator
            name: torch_cifar100
            path: original_data/
          sample:
            _target_: deckard.base.data.sampler.SklearnDataSampler
            random_state: 0
            stratify: true
            test_size: 100
            train_size: 100
          sklearn_pipeline:
            _target_: deckard.base.data.sklearn_pipeline.SklearnDataPipeline
            preprocessor:
              name: sklearn.preprocessing.StandardScaler
              with_mean: true
              with_std: true
        files:
          _target_: deckard.base.files.FileConfig
          adv_predictions_file: adv_predictions.json
          attack_dir: attacks
          attack_file: attack
          attack_type: .pkl
          data_dir: data
          data_file: data
          data_type: .pkl
          directory: cifar100
          model_dir: models
          model_file: model
          model_type: .pt
          name: default
          params_file: params.yaml
          predictions_file: predictions.json
          reports: reports
          score_dict_file: score_dict.json
        model:
          _target_: deckard.base.model.Model
          art:
            _target_: deckard.base.model.art_pipeline.ArtPipeline
            data:
              _target_: deckard.base.data.Data
              generate:
                _target_: deckard.base.data.generator.DataGenerator
                name: torch_cifar100
                path: original_data/
              sample:
                _target_: deckard.base.data.sampler.SklearnDataSampler
                random_state: 0
                stratify: true
                test_size: 100
                train_size: 100
              sklearn_pipeline:
                _target_: deckard.base.data.sklearn_pipeline.SklearnDataPipeline
                preprocessor:
                  name: sklearn.preprocessing.StandardScaler
                  with_mean: true
                  with_std: true
            initialize:
              clip_values:
              - 0
              - 255
              criterion:
                name: torch.nn.CrossEntropyLoss
              optimizer:
                lr: 0.01
                momentum: 0.9
                name: torch.optim.SGD
            library: pytorch
          data:
            _target_: deckard.base.data.Data
            generate:
              _target_: deckard.base.data.generator.DataGenerator
              name: torch_cifar100
              path: original_data/
            sample:
              _target_: deckard.base.data.sampler.SklearnDataSampler
              random_state: 0
              stratify: true
              test_size: 100
              train_size: 100
            sklearn_pipeline:
              _target_: deckard.base.data.sklearn_pipeline.SklearnDataPipeline
              preprocessor:
                name: sklearn.preprocessing.StandardScaler
                with_mean: true
                with_std: true
          init:
            _target_: deckard.base.model.ModelInitializer
            name: torch_example.ResNet18
            num_channels: 3
            num_classes: 100
          library: pytorch
          trainer:
            batch_size: 1024
            nb_epoch: 1
        scorers:
          _target_: deckard.base.scorer.ScorerDict
          accuracy:
            _target_: deckard.base.scorer.ScorerConfig
            direction: maximize
            name: sklearn.metrics.accuracy_score
          log_loss:
            _target_: deckard.base.scorer.ScorerConfig
            direction: minimize
            name: sklearn.metrics.log_loss
    outs:
    - path: cifar100/reports/train/default/score_dict.json
      hash: md5
      md5: 0d7b5dc69390c3934014592f0ca69754
      size: 752
  attack:
    cmd: python -m deckard.layers.experiment attack --config_file cifar100.yaml
    deps:
    - path: params.yaml
      hash: md5
      md5: 3d21fe2d291f5b25fa57e440ddaa9d5e
      size: 7525
    params:
      params.yaml:
        attack:
          _target_: deckard.base.attack.Attack
          attack_size: 10
          data:
            _target_: deckard.base.data.Data
            generate:
              _target_: deckard.base.data.generator.DataGenerator
              name: torch_cifar100
              path: original_data/
            sample:
              _target_: deckard.base.data.sampler.SklearnDataSampler
              random_state: 0
              stratify: true
              test_size: 100
              train_size: 100
            sklearn_pipeline:
              _target_: deckard.base.data.sklearn_pipeline.SklearnDataPipeline
              preprocessor:
                name: sklearn.preprocessing.StandardScaler
                with_mean: true
                with_std: true
          init:
            _target_: deckard.base.attack.AttackInitializer
            eps: 0.99
            eps_step: 0.099
            model:
              _target_: deckard.base.model.Model
              art:
                _target_: deckard.base.model.art_pipeline.ArtPipeline
                data:
                  _target_: deckard.base.data.Data
                  generate:
                    _target_: deckard.base.data.generator.DataGenerator
                    name: torch_cifar100
                    path: original_data/
                  sample:
                    _target_: deckard.base.data.sampler.SklearnDataSampler
                    random_state: 0
                    stratify: true
                    test_size: 100
                    train_size: 100
                  sklearn_pipeline:
                    _target_: deckard.base.data.sklearn_pipeline.SklearnDataPipeline
                    preprocessor:
                      name: sklearn.preprocessing.StandardScaler
                      with_mean: true
                      with_std: true
                initialize:
                  clip_values:
                  - 0
                  - 255
                  criterion:
                    name: torch.nn.CrossEntropyLoss
                  optimizer:
                    lr: 0.01
                    momentum: 0.9
                    name: torch.optim.SGD
                library: pytorch
              data:
                _target_: deckard.base.data.Data
                generate:
                  _target_: deckard.base.data.generator.DataGenerator
                  name: torch_cifar100
                  path: original_data/
                sample:
                  _target_: deckard.base.data.sampler.SklearnDataSampler
                  random_state: 0
                  stratify: true
                  test_size: 100
                  train_size: 100
                sklearn_pipeline:
                  _target_: deckard.base.data.sklearn_pipeline.SklearnDataPipeline
                  preprocessor:
                    name: sklearn.preprocessing.StandardScaler
                    with_mean: true
                    with_std: true
              init:
                _target_: deckard.base.model.ModelInitializer
                name: torch_example.ResNet18
                num_channels: 3
                num_classes: 100
              library: pytorch
              trainer:
                batch_size: 1024
                nb_epoch: 1
            name: art.attacks.evasion.FastGradientMethod
          method: evasion
          model:
            _target_: deckard.base.model.Model
            art:
              _target_: deckard.base.model.art_pipeline.ArtPipeline
              data:
                _target_: deckard.base.data.Data
                generate:
                  _target_: deckard.base.data.generator.DataGenerator
                  name: torch_cifar100
                  path: original_data/
                sample:
                  _target_: deckard.base.data.sampler.SklearnDataSampler
                  random_state: 0
                  stratify: true
                  test_size: 100
                  train_size: 100
                sklearn_pipeline:
                  _target_: deckard.base.data.sklearn_pipeline.SklearnDataPipeline
                  preprocessor:
                    name: sklearn.preprocessing.StandardScaler
                    with_mean: true
                    with_std: true
              initialize:
                clip_values:
                - 0
                - 255
                criterion:
                  name: torch.nn.CrossEntropyLoss
                optimizer:
                  lr: 0.01
                  momentum: 0.9
                  name: torch.optim.SGD
              library: pytorch
            data:
              _target_: deckard.base.data.Data
              generate:
                _target_: deckard.base.data.generator.DataGenerator
                name: torch_cifar100
                path: original_data/
              sample:
                _target_: deckard.base.data.sampler.SklearnDataSampler
                random_state: 0
                stratify: true
                test_size: 100
                train_size: 100
              sklearn_pipeline:
                _target_: deckard.base.data.sklearn_pipeline.SklearnDataPipeline
                preprocessor:
                  name: sklearn.preprocessing.StandardScaler
                  with_mean: true
                  with_std: true
            init:
              _target_: deckard.base.model.ModelInitializer
              name: torch_example.ResNet18
              num_channels: 3
              num_classes: 100
            library: pytorch
            trainer:
              batch_size: 1024
              nb_epoch: 1
        data:
          _target_: deckard.base.data.Data
          generate:
            _target_: deckard.base.data.generator.DataGenerator
            name: torch_cifar100
            path: original_data/
          sample:
            _target_: deckard.base.data.sampler.SklearnDataSampler
            random_state: 0
            stratify: true
            test_size: 100
            train_size: 100
          sklearn_pipeline:
            _target_: deckard.base.data.sklearn_pipeline.SklearnDataPipeline
            preprocessor:
              name: sklearn.preprocessing.StandardScaler
              with_mean: true
              with_std: true
        files:
          _target_: deckard.base.files.FileConfig
          adv_predictions_file: adv_predictions.json
          attack_dir: attacks
          attack_file: attack
          attack_type: .pkl
          data_dir: data
          data_file: data
          data_type: .pkl
          directory: cifar100
          model_dir: models
          model_file: model
          model_type: .pt
          name: default
          params_file: params.yaml
          predictions_file: predictions.json
          reports: reports
          score_dict_file: score_dict.json
        model:
          _target_: deckard.base.model.Model
          art:
            _target_: deckard.base.model.art_pipeline.ArtPipeline
            data:
              _target_: deckard.base.data.Data
              generate:
                _target_: deckard.base.data.generator.DataGenerator
                name: torch_cifar100
                path: original_data/
              sample:
                _target_: deckard.base.data.sampler.SklearnDataSampler
                random_state: 0
                stratify: true
                test_size: 100
                train_size: 100
              sklearn_pipeline:
                _target_: deckard.base.data.sklearn_pipeline.SklearnDataPipeline
                preprocessor:
                  name: sklearn.preprocessing.StandardScaler
                  with_mean: true
                  with_std: true
            initialize:
              clip_values:
              - 0
              - 255
              criterion:
                name: torch.nn.CrossEntropyLoss
              optimizer:
                lr: 0.01
                momentum: 0.9
                name: torch.optim.SGD
            library: pytorch
          data:
            _target_: deckard.base.data.Data
            generate:
              _target_: deckard.base.data.generator.DataGenerator
              name: torch_cifar100
              path: original_data/
            sample:
              _target_: deckard.base.data.sampler.SklearnDataSampler
              random_state: 0
              stratify: true
              test_size: 100
              train_size: 100
            sklearn_pipeline:
              _target_: deckard.base.data.sklearn_pipeline.SklearnDataPipeline
              preprocessor:
                name: sklearn.preprocessing.StandardScaler
                with_mean: true
                with_std: true
          init:
            _target_: deckard.base.model.ModelInitializer
            name: torch_example.ResNet18
            num_channels: 3
            num_classes: 100
          library: pytorch
          trainer:
            batch_size: 1024
            nb_epoch: 1
        scorers:
          _target_: deckard.base.scorer.ScorerDict
          accuracy:
            _target_: deckard.base.scorer.ScorerConfig
            direction: maximize
            name: sklearn.metrics.accuracy_score
          log_loss:
            _target_: deckard.base.scorer.ScorerConfig
            direction: minimize
            name: sklearn.metrics.log_loss
    outs:
    - path: cifar100/attacks/attack.pkl
      hash: md5
      md5: 9ba2fe1196bcbf2379946652865ace9b
      size: 123046
    - path: cifar100/data/data.pkl
      hash: md5
      md5: 594841554582ca2a77c0a73c7e8842cb
      size: 2537889
    - path: cifar100/models/model.optimizer.pt
      hash: md5
      md5: 56e67611c3cc7e9c416c91cc6a6b2640
      size: 44989358
    - path: cifar100/models/model.pt
      hash: md5
      md5: 7a728af7fb0ca90e720cb5d25fee89af
      size: 44995266
    - path: cifar100/reports/attack/default/adv_predictions.json
      hash: md5
      md5: c3a0272213db04f9dbbb2c509abfe597
      size: 20979
    - path: cifar100/reports/attack/default/predictions.json
      hash: md5
      md5: 3b039c0276e39808bab5b4363e26a062
      size: 214402
    - path: cifar100/reports/attack/default/score_dict.json
      hash: md5
      md5: 44d2a553a36ce676b9a1a445345bd88c
      size: 1168
  attacks@ResNet18:
    cmd: bash attacks.sh ++attack.attack_size=100 ++model.init.name=torch_example.ResNet18
      stage=attack ++hydra.sweeper.storage=sqlite:///mnist/reports/attack/ResNet18.db
      ++direction="[maximize,minimize,minimize]" ++optimizers="[adv_accuracy,adv_fit_time,adv_predict_time]"
      --config-name mnist.yaml
    deps:
    - path: attacks.sh
      hash: md5
      md5: 0e9c2eaa696b28e79478fc38cea8806c
      size: 2931
    - path: mnist/reports/attack/default/score_dict.json
      hash: md5
      md5: 39095486e24a8ec7e68d206ba8aceb52
      size: 433
    - path: models.sh
      hash: md5
      md5: 1937e58bedac027034aea7d4a5712407
      size: 1380
    outs:
    - path: mnist/reports/attack/ResNet18.db
      hash: md5
      md5: 25aabf8a3c319f0a319098d1564976f2
      size: 131072
  install_deckard:
    cmd: python -m pip install -e ../../ && python -m pip install torch torchvision
    deps:
    - path: ../../setup.py
      hash: md5
      md5: df6acc4b15670c55b1aa9f23f1cc892f
      size: 4886
    outs:
    - path: ../../deckard.egg-info
      hash: md5
      md5: 244a42d703838a2a3ffeb80ea7415b3f.dir
      size: 13515
      nfiles: 5
  install_torch:
    cmd: python -m pip install torch torchvision
  parse_params:
    cmd: python -m deckard.layers.parse --config_file cifar100.yaml
    deps:
    - path: conf/
      hash: md5
      md5: 8344c81b7bbf726ade0803d83eac19c6.dir
      size: 124687
      nfiles: 21
    outs:
    - path: params.yaml
      hash: md5
      md5: 3d21fe2d291f5b25fa57e440ddaa9d5e
      size: 7525
  install_redis:
    cmd: bash redis.sh
    outs:
    - path: .bashrc
      hash: md5
      md5: 088676c495eacf10a7b51ebf17c9012c
      size: 40
  models@ResNet18:
    cmd: bash models.sh ++model.init.name=torch_example.ResNet18 stage=train ++hydra.sweeper.storage=sqlite:///mnist/reports/train/ResNet18.db
      --config-name mnist.yaml
    deps:
    - path: mnist/models/model.optimizer.pt
      hash: md5
      md5: 4d9e657ab48d018cf0f8ed1aadac7464
      size: 44779566
    - path: mnist/models/model.pt
      hash: md5
      md5: 3f5aae5433e60ee931e4e01dc07c1682
      size: 44785474
    - path: models.sh
      hash: md5
      md5: 1937e58bedac027034aea7d4a5712407
      size: 1380
    outs:
    - path: mnist/reports/train/ResNet18.db
      hash: md5
      md5: 215c80cf43517f786ac05014e3193c64
      size: 110592
  models@mnist:
    cmd: bash models.sh  stage=train ++hydra.sweeper.storage=sqlite:///mnist/reports/train/mnist.db
      --config-name mnist.yaml
    deps:
    - path: mnist/models/model.optimizer.pt
      hash: md5
      md5: 4c2b10c966d0d7b40563303215d2171e
      size: 44779566
    - path: mnist/models/model.pt
      hash: md5
      md5: 7c8df9f88ecbb0ced92c30674818d4bd
      size: 44785474
    - path: models.sh
      hash: md5
      md5: 6b985d4aeb1e8c170cda9752974a664c
      size: 208
    outs:
    - path: mnist/reports/train/mnist.db
      hash: md5
      md5: 663076070c6766ac69d497c0934591ab
      size: 122880
  attacks@mnist:
    cmd: python -m deckard.layers.optimise  $@ --multirun ++attack.attack_size=100
      stage=attack ++hydra.sweeper.storage=sqlite:///mnist/reports/attack/mnist.db  --config-name
      mnist.yaml
    deps:
    - path: mnist/reports/attack/default/score_dict.json
      hash: md5
      md5: a856df313ca8316d37f6ffad821d1fc9
      size: 1168
    outs:
    - path: mnist/reports/attack/mnist.db
      hash: md5
      md5: 16a0df62da4de50ebe7a6c50a0e5ddca
      size: 147456
  compile@attack:
    cmd: python -m deckard.layers.compile --report_folder mnist/reports/attack --results_file
      mnist/reports/attack.csv
    deps:
    - path: mnist/reports/attack/
      hash: md5
      md5: 50da2bd5f58cde6199389e7852f7d47f.dir
      size: 655782
      nfiles: 59
    - path: mnist/reports/attack/mnist.db
      hash: md5
      md5: 6eb630e8fda03e569bb354779c00f2a6
      size: 131072
    outs:
    - path: mnist/reports/attack.csv
      hash: md5
      md5: 7b9597518294657eaf40254e114e5f8e
      size: 85745
  attacks@cifar10:
    cmd: python -m deckard.layers.optimise  $@ --multirun ++attack.attack_size=100
      stage=attack ++hydra.sweeper.storage=sqlite:///cifar10/reports/attack/cifar10.db  --config-name
      cifar10.yaml
    deps:
    - path: cifar10/reports/attack/default/score_dict.json
      hash: md5
      md5: 01600d66887a27d7c7efc29a43725f9c
      size: 1166
    outs:
    - path: cifar10/reports/attack/cifar10.db
      hash: md5
      md5: a95d6074e3866d68514adf70034d1fde
      size: 135168
  attacks@cifar100:
    cmd: python -m deckard.layers.optimise  $@ --multirun ++attack.attack_size=100
      stage=attack ++hydra.sweeper.storage=sqlite:///cifar100/reports/attack/cifar100.db  --config-name
      cifar100.yaml
    deps:
    - path: cifar100/reports/attack/default/score_dict.json
      hash: md5
      md5: 44d2a553a36ce676b9a1a445345bd88c
      size: 1168
    outs:
    - path: cifar100/reports/attack/cifar100.db
      hash: md5
      md5: 8dba184e770eac33523ffeccd55580d6
      size: 135168
